{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import KBinsDiscretizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-29T14:03:35.146092Z","iopub.execute_input":"2024-12-29T14:03:35.146336Z","iopub.status.idle":"2024-12-29T14:03:36.125197Z","shell.execute_reply.started":"2024-12-29T14:03:35.146303Z","shell.execute_reply":"2024-12-29T14:03:36.124278Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s4e12/train.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e12/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T14:03:36.126097Z","iopub.execute_input":"2024-12-29T14:03:36.126576Z","iopub.status.idle":"2024-12-29T14:03:44.811383Z","shell.execute_reply.started":"2024-12-29T14:03:36.126550Z","shell.execute_reply":"2024-12-29T14:03:44.810690Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def date_trans(df):\n    df['Policy Start Date']= pd.to_datetime(df['Policy Start Date'])\n    df['Year'] = df['Policy Start Date'].dt.year\n    df['Day'] = df['Policy Start Date'].dt.day\n    df['Month'] = df['Policy Start Date'].dt.month\n    df['DayOfWeek'] = df['Policy Start Date'].dt.dayofweek\n    df.drop('Policy Start Date' , axis =1, inplace = True)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T14:03:44.812110Z","iopub.execute_input":"2024-12-29T14:03:44.812324Z","iopub.status.idle":"2024-12-29T14:03:44.816825Z","shell.execute_reply.started":"2024-12-29T14:03:44.812306Z","shell.execute_reply":"2024-12-29T14:03:44.815987Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_df = date_trans(train_df)\ntest_df = date_trans(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T14:03:44.819028Z","iopub.execute_input":"2024-12-29T14:03:44.819315Z","iopub.status.idle":"2024-12-29T14:03:46.126965Z","shell.execute_reply.started":"2024-12-29T14:03:44.819284Z","shell.execute_reply":"2024-12-29T14:03:46.126063Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T14:03:46.128043Z","iopub.execute_input":"2024-12-29T14:03:46.128327Z","iopub.status.idle":"2024-12-29T14:03:46.132572Z","shell.execute_reply.started":"2024-12-29T14:03:46.128304Z","shell.execute_reply":"2024-12-29T14:03:46.131464Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"for col in train_df.select_dtypes(include='object').columns:\n    train_df[col] = train_df[col].astype('category')\nfor col in test_df.select_dtypes(include='object').columns:\n    test_df[col] = test_df[col].astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T14:03:46.133493Z","iopub.execute_input":"2024-12-29T14:03:46.133746Z","iopub.status.idle":"2024-12-29T14:03:48.136197Z","shell.execute_reply.started":"2024-12-29T14:03:46.133719Z","shell.execute_reply":"2024-12-29T14:03:48.135302Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\ndef log_transform(y):\n    \"\"\"\n    Apply log transformation safely, handling zero and negative values\n    \"\"\"\n    return np.log1p(y)\n\ndef inverse_log_transform(y_log):\n    \"\"\"\n    Revert log transformation\n    \"\"\"\n    return np.expm1(y_log)\n\ndef rmsle(y_true, y_pred):\n    \"\"\"\n    Calculate Root Mean Squared Logarithmic Error\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\n# Example dataset (replace with your dataset)\nnp.random.seed(42)\ndf = train_df\n\n# Find the target column (assumes it contains 'Premium' or 'Amount')\ntarget_column = [col for col in df.columns if 'premium' in col.lower() or 'amount' in col.lower()]\nif not target_column:\n    raise ValueError(\"Could not find target column. Please specify the column name for premium/amount.\")\ntarget_column = target_column[0]\n\n# Identify column types\nnumeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# Remove target variable from features\nif target_column in numeric_features:\n    numeric_features.remove(target_column)\nif target_column in categorical_features:\n    categorical_features.remove(target_column)\n\n# Features and target\nX = df.drop(columns=[target_column])\ny = df[target_column]\n\n# Log transform the target variable\ny_log = log_transform(y)\n\n# Stratify target by binning into discrete intervals\nbinner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\ny_binned = binner.fit_transform(y_log.values.reshape(-1, 1)).astype(int).ravel()\n\n# Test set (replace with your actual test set)\nX_test = test_df\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', SimpleImputer(strategy='median'), numeric_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n    ]\n)\n\n# LinearRegression model parameters (no hyperparameters to tune)\nmodel = LinearRegression()\n\n# Stratified K-Fold\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Arrays to store predictions\noof_predictions_log = np.zeros(len(X))  # Out-of-fold predictions in log space\noof_predictions = np.zeros(len(X))  # Out-of-fold predictions in original space\n\n# Prepare for cross-validation\ntest_preds_per_fold_log = np.zeros((len(X_test), n_splits))  # Store test predictions per fold in log space\ntest_preds_per_fold = np.zeros((len(X_test), n_splits))  # Store test predictions per fold in original space\nfold_oof_results = []  # To store ID, target, and OOF predictions\n\n# Cross-validation loop\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X, y_binned)):\n    print(f\"Fold {fold + 1}/{n_splits}\")\n    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train_log, y_valid_log = y_log.iloc[train_idx], y_log.iloc[valid_idx]\n    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n    \n    # Model pipeline with preprocessing and regressor\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    \n    # Fit the model\n    pipeline.fit(X_train, y_train_log)\n    \n    # Predictions in log space\n    oof_predictions_log[valid_idx] = pipeline.predict(X_valid)\n    \n    # Convert log predictions back to original space\n    oof_predictions[valid_idx] = inverse_log_transform(oof_predictions_log[valid_idx])\n\n    fold_rmsle = rmsle(y.iloc[valid_idx], oof_predictions[valid_idx])\n    print(f\"Fold {fold + 1} RMSLE: {fold_rmsle:.4f}\")\n    \n    # Store fold results with ID, target, and OOF predictions\n    fold_result = pd.DataFrame({\n        'ID': X.index[valid_idx],       # Assuming X has an index as IDs\n        'Actual': y.iloc[valid_idx],    # Actual target values\n        'OOF_Pred_LR': oof_predictions[valid_idx],    # OOF predictions\n        'Fold': fold + 1               # Fold number\n    })\n    fold_oof_results.append(fold_result)\n    \n    # Test set predictions for this fold\n    test_preds_per_fold_log[:, fold] = pipeline.predict(X_test)\n    test_preds_per_fold[:, fold] = inverse_log_transform(test_preds_per_fold_log[:, fold])\n\n# Combine OOF results\noof_results_df = pd.concat(fold_oof_results, axis=0, ignore_index=True)\n\n# Average predictions on test data\nfinal_test_predictions = test_preds_per_fold.mean(axis=1)\n\n# Evaluate OOF predictions\noof_mse = mean_squared_error(y, oof_predictions)\noof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n\nprint(f\"OOF Mean Squared Error: {oof_mse:.4f}\")\nprint(f\"OOF Root Mean Squared Log Error: {oof_rmsle:.4f}\")\n\nprint(\"Final Test Predictions:\", final_test_predictions)\nprint(oof_results_df.head())\n\n\nsub = pd.read_csv('/kaggle/input/playground-series-s4e12/sample_submission.csv')\nsub['Premium Amount'] = final_test_predictions\nsub.to_csv('lrsubmission.csv', index=False)\nsub.head()\noof_results_df.to_csv('oof_lr.csv',index = False)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T14:03:48.137080Z","iopub.execute_input":"2024-12-29T14:03:48.137318Z","iopub.status.idle":"2024-12-29T14:04:37.090689Z","shell.execute_reply.started":"2024-12-29T14:03:48.137296Z","shell.execute_reply":"2024-12-29T14:04:37.089876Z"}},"outputs":[{"name":"stdout","text":"Fold 1/5\nFold 1 RMSLE: 1.0891\nFold 2/5\nFold 2 RMSLE: 1.0878\nFold 3/5\nFold 3 RMSLE: 1.0882\nFold 4/5\nFold 4 RMSLE: 1.0885\nFold 5/5\nFold 5 RMSLE: 1.0884\nOOF Mean Squared Error: 884219.2629\nOOF Root Mean Squared Log Error: 1.0884\nFinal Test Predictions: [907.22219971 582.83524658 674.65823017 ... 707.11061863 751.87394019\n 737.56303972]\n   ID  Actual  OOF_Pred_LR  Fold\n0   4  2022.0   667.492085     1\n1  15   849.0   779.015390     1\n2  21  2670.0   654.119606     1\n3  30   641.0   789.055631     1\n4  34  2152.0   649.752725     1\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        id  Premium Amount\n0  1200000      907.222200\n1  1200001      582.835247\n2  1200002      674.658230\n3  1200003      643.867759\n4  1200004      696.832252","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Premium Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1200000</td>\n      <td>907.222200</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1200001</td>\n      <td>582.835247</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1200002</td>\n      <td>674.658230</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1200003</td>\n      <td>643.867759</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1200004</td>\n      <td>696.832252</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# sub = pd.read_csv('/kaggle/input/playground-series-s4e12/sample_submission.csv')\n# sub['Premium Amount'] = final_test_predictions\n# sub.to_csv('lgsubmission.csv', index=False)\n# sub.head()\n# oof_results_df.to_csv('oof_lgbm.csv',index = False)\n# sub.head()\n\nsub = pd.read_csv('/kaggle/input/playground-series-s4e12/sample_submission.csv')\nsub['Premium Amount'] = final_test_predictions\nsub.to_csv('lrsubmission.csv', index=False)\nsub.head()\noof_results_df.to_csv('oof_lr.csv',index = False)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T14:06:44.634805Z","iopub.execute_input":"2024-12-29T14:06:44.635180Z","iopub.status.idle":"2024-12-29T14:06:48.761710Z","shell.execute_reply.started":"2024-12-29T14:06:44.635155Z","shell.execute_reply":"2024-12-29T14:06:48.760974Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        id  Premium Amount\n0  1200000      907.222200\n1  1200001      582.835247\n2  1200002      674.658230\n3  1200003      643.867759\n4  1200004      696.832252","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Premium Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1200000</td>\n      <td>907.222200</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1200001</td>\n      <td>582.835247</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1200002</td>\n      <td>674.658230</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1200003</td>\n      <td>643.867759</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1200004</td>\n      <td>696.832252</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_squared_log_error\n# from catboost import CatBoostRegressor\n# import optuna\n\n# # Log transform function\n# def log_transform(y):\n#     return np.log1p(y)  # log(1+y) to handle zero values\n\n# # Inverse log transform function\n# def inverse_log_transform(y_log):\n#     return np.expm1(y_log)  # exp(y) - 1 to revert log1p\n\n# # Prepare data\n# X = train_data.drop('Premium Amount', axis=1)\n# y = train_data['Premium Amount']\n\n# # Log transform the target variable\n# y_log = log_transform(y)\n\n# # Assuming all columns except 'Premium Amount' are numerical; update this as needed.\n# categorical_features = [i for i, col in enumerate(X.columns) if X[col].dtype == 'object']\n\n# def objective(trial):\n#     # Hyperparameter space to search\n#     params = {\n#         \"iterations\": trial.suggest_int(\"iterations\", 100, 10000, step=100),\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n#         \"depth\": trial.suggest_int(\"depth\", 3, 10),\n#         \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 10.0, log=True),\n#         \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n#         \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 10.0),\n#         \"task_type\": \"GPU\",  # Use GPU for training\n#         \"devices\": \"0\",  # Use first GPU (Kaggle provides a single GPU, so \"0\" is the correct option)\n#         \"eval_metric\": \"MSLE\",  # Use MSLE instead of RMSLE\n#         \"loss_function\": \"RMSE\",  # Retaining RMSE for training\n#         \"early_stopping_rounds\": 50,\n#         \"cat_features\": categorical_features\n#     }\n    \n#     # Split data into train and validation sets\n#     X_train, X_valid, y_train, y_valid = train_test_split(X, y_log, test_size=0.2, random_state=42)\n    \n#     # Initialize the model with the given parameters\n#     model = CatBoostRegressor(**params)\n    \n#     # Fit the model on log-transformed target\n#     model.fit(\n#         X_train, y_train,\n#         eval_set=[(X_valid, y_valid)],\n#         verbose=False\n#     )\n    \n#     # Predict on the validation set and inverse transform\n#     y_pred_log = model.predict(X_valid)\n#     y_pred = inverse_log_transform(y_pred_log)\n#     y_valid_original = inverse_log_transform(y_valid)\n    \n#     # Calculate RMSLE on original scale\n#     score = np.sqrt(mean_squared_log_error(y_valid_original, y_pred))\n    \n#     return score\n\n# # Optuna Study for Hyperparameter Tuning\n# study = optuna.create_study(direction=\"minimize\")  # We want to minimize RMSLE\n# study.optimize(objective, n_trials=50)\n\n# # Print the best parameters and score\n# print(\"Best RMSLE:\", study.best_value)\n# print(\"Best Hyperparameters:\", study.best_params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T14:04:41.276178Z","iopub.execute_input":"2024-12-29T14:04:41.276379Z","iopub.status.idle":"2024-12-29T14:04:41.280260Z","shell.execute_reply.started":"2024-12-29T14:04:41.276362Z","shell.execute_reply":"2024-12-29T14:04:41.279451Z"}},"outputs":[],"execution_count":9}]}