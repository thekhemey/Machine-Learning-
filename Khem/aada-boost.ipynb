{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\nfrom statistics import mean\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport optuna\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import mean_squared_log_error","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:35:12.538157Z","iopub.execute_input":"2024-12-30T06:35:12.538684Z","iopub.status.idle":"2024-12-30T06:35:13.351574Z","shell.execute_reply.started":"2024-12-30T06:35:12.53857Z","shell.execute_reply":"2024-12-30T06:35:13.350398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s4e12/train.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e12/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:35:13.353005Z","iopub.execute_input":"2024-12-30T06:35:13.353427Z","iopub.status.idle":"2024-12-30T06:35:25.172148Z","shell.execute_reply.started":"2024-12-30T06:35:13.353395Z","shell.execute_reply":"2024-12-30T06:35:25.170941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def date_trans(df):\n    df['Policy Start Date']= pd.to_datetime(df['Policy Start Date'])\n    df['Year'] = df['Policy Start Date'].dt.year\n    df['Day'] = df['Policy Start Date'].dt.day\n    df['Month'] = df['Policy Start Date'].dt.month\n    df['DayOfWeek'] = df['Policy Start Date'].dt.dayofweek\n    df.drop('Policy Start Date' , axis =1, inplace = True)\n    return df\n\ntrain_df = date_trans(train_df)\ntest_df = date_trans(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:35:25.174488Z","iopub.execute_input":"2024-12-30T06:35:25.174804Z","iopub.status.idle":"2024-12-30T06:35:26.892985Z","shell.execute_reply.started":"2024-12-30T06:35:25.174777Z","shell.execute_reply":"2024-12-30T06:35:26.891159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:35:26.89456Z","iopub.execute_input":"2024-12-30T06:35:26.89506Z","iopub.status.idle":"2024-12-30T06:35:26.902071Z","shell.execute_reply.started":"2024-12-30T06:35:26.894995Z","shell.execute_reply":"2024-12-30T06:35:26.900294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assume train and test are already loaded as DataFrames\nfor col in train_df.select_dtypes(include='object').columns:\n    train_df[col] = train_df[col].astype('category')\nfor col in test_df.select_dtypes(include='object').columns:\n    test_df[col] = test_df[col].astype('category')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:35:26.903579Z","iopub.execute_input":"2024-12-30T06:35:26.904036Z","iopub.status.idle":"2024-12-30T06:35:29.309411Z","shell.execute_reply.started":"2024-12-30T06:35:26.903997Z","shell.execute_reply":"2024-12-30T06:35:29.308212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"AADABOOST ","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\n# from sklearn.metrics import mean_squared_error, mean_squared_log_error\n# from sklearn.ensemble import AdaBoostRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.impute import SimpleImputer\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n\n# # Set random seed\n# np.random.seed(42)\n\n# # Assume train_df and test_df are your DataFrames\n# df = train_df\n\n# # Features and target\n# X = df.drop('Premium Amount', axis=1)\n# y = df['Premium Amount']\n\n# # Log transform the target\n# y_log = np.log1p(y)\n\n# # Stratify target by binning into discrete intervals\n# binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n# y_binned_log = binner.fit_transform(y_log.values.reshape(-1, 1)).astype(int).ravel()\n\n# # Identify categorical and numerical columns\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n# categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# # Create imputers for numerical and categorical columns\n# numerical_imputer = SimpleImputer(strategy='mean')  # Use mean for numerical features\n# categorical_imputer = SimpleImputer(strategy='most_frequent')  # Use most frequent for categorical features\n# categorical_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)  # One hot encoding for categorical features\n\n# # Preprocessing pipeline\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_imputer, numerical_cols),\n#         ('cat', Pipeline([\n#             ('imputer', categorical_imputer),\n#             ('encoder', categorical_encoder)\n#         ]), categorical_cols)\n#     ]\n# )\n\n# # Stratified K-Fold\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # Arrays to store predictions\n# oof_predictions = np.zeros(len(X))  # Out-of-fold predictions\n# test_predictions = np.zeros(len(X))  # Test predictions for each fold\n\n# # Test set\n# X_test = test_df\n# test_preds_per_fold = np.zeros((len(X_test), n_splits))  # Store test predictions per fold\n# fold_oof_results = []  # To store ID, target, and OOF predictions\n\n# # Cross-validation loop\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y_binned_log)):\n#     print(f\"Fold {fold + 1}/{n_splits}\")\n    \n#     # Split data\n#     X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n#     y_train, y_valid = y_log.iloc[train_idx], y_log.iloc[valid_idx]\n    \n#     # Apply preprocessing to both train and valid sets\n#     X_train_transformed = preprocessor.fit_transform(X_train)\n#     X_valid_transformed = preprocessor.transform(X_valid)\n    \n#     # Model\n#     model = AdaBoostRegressor(\n#         base_estimator=DecisionTreeRegressor(max_depth=5),\n#         n_estimators=100,\n#         learning_rate=0.1,\n#         random_state=42\n#     )\n#     model.fit(X_train_transformed, y_train)\n    \n#     # Predictions (log transformed)\n#     log_oof_preds = model.predict(X_valid_transformed)\n    \n#     # Revert log transformation\n#     oof_predictions[valid_idx] = np.expm1(log_oof_preds)\n\n#     fold_rmsle = np.sqrt(mean_squared_log_error(y.iloc[valid_idx], oof_predictions[valid_idx]))\n#     print(f\"Fold {fold + 1} RMSLE: {fold_rmsle:.4f}\")\n    \n#     # Store fold results\n#     fold_result = pd.DataFrame({\n#         'ID': X.index[valid_idx],\n#         'Actual': y.iloc[valid_idx],\n#         'OOF_Pred': oof_predictions[valid_idx],\n#         'Fold': fold + 1\n#     })\n#     fold_oof_results.append(fold_result)\n    \n#     # Test set predictions (transform test data)\n#     X_test_transformed = preprocessor.transform(X_test)\n#     log_test_preds = model.predict(X_test_transformed)\n#     test_preds_per_fold[:, fold] = np.expm1(log_test_preds)\n\n# # Combine fold results\n# oof_results_df = pd.concat(fold_oof_results, axis=0, ignore_index=True)\n\n# # Average predictions on test data\n# final_test_predictions = test_preds_per_fold.mean(axis=1)\n\n# # Evaluate OOF predictions\n# oof_mse = mean_squared_error(y, oof_predictions)\n# oof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n\n# print(f\"OOF Mean Squared Error: {oof_mse:.4f}\")\n# print(f\"OOF Root Mean Squared Log Error: {oof_rmsle:.4f}\")\n\n# # Output predictions\n# print(\"Final Test Predictions:\", final_test_predictions)\n# print(oof_results_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:35:29.310505Z","iopub.execute_input":"2024-12-30T06:35:29.310927Z","iopub.status.idle":"2024-12-30T06:35:29.318323Z","shell.execute_reply.started":"2024-12-30T06:35:29.310865Z","shell.execute_reply":"2024-12-30T06:35:29.315903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sub = pd.read_csv('/kaggle/input/playground-series-s4e12/sample_submission.csv')\n# sub['Premium Amount'] = final_test_predictions\n# sub.to_csv('submission.csv', index=False)\n# oof_results_df.to_csv('oof_aaba.csv',index = False)\n# sub.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:35:29.319951Z","iopub.execute_input":"2024-12-30T06:35:29.320286Z","iopub.status.idle":"2024-12-30T06:35:29.355104Z","shell.execute_reply.started":"2024-12-30T06:35:29.320257Z","shell.execute_reply":"2024-12-30T06:35:29.353716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Set random seed\nnp.random.seed(42)\n\n# Assume train_df and test_df are your DataFrames\ndf = train_df\n\n# Features and target\nX = df.drop('Premium Amount', axis=1)\ny = df['Premium Amount']\n\n# Log transform the target\ny_log = np.log1p(y)\n\n# Stratify target by binning into discrete intervals\nbinner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\ny_binned_log = binner.fit_transform(y_log.values.reshape(-1, 1)).astype(int).ravel()\n\n# Identify categorical and numerical columns\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# Create imputers for numerical and categorical columns\nnumerical_imputer = SimpleImputer(strategy='mean')  # Use mean for numerical features\ncategorical_imputer = SimpleImputer(strategy='most_frequent')  # Use most frequent for categorical features\ncategorical_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)  # One hot encoding for categorical features\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_imputer, numerical_cols),\n        ('cat', Pipeline([\n            ('imputer', categorical_imputer),\n            ('encoder', categorical_encoder)\n        ]), categorical_cols)\n    ]\n)\n\n# Stratified K-Fold\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Arrays to store predictions\noof_predictions = np.zeros(len(X))  # Out-of-fold predictions\ntest_predictions = np.zeros(len(X))  # Test predictions for each fold\n\n# Test set\nX_test = test_df\ntest_preds_per_fold = np.zeros((len(X_test), n_splits))  # Store test predictions per fold\nfold_oof_results = []  # To store ID, target, and OOF predictions\n\n# Best hyperparameters\nbest_params = {\n    'tree_depth': 6,\n    'min_samples_split': 7,\n    'n_estimators': 121,\n    'learning_rate': 0.005829169740875408,\n    'loss': 'square'\n}\n\n# Cross-validation loop\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X, y_binned_log)):\n    print(f\"Fold {fold + 1}/{n_splits}\")\n    \n    # Split data\n    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_valid = y_log.iloc[train_idx], y_log.iloc[valid_idx]\n    \n    # Apply preprocessing to both train and valid sets\n    X_train_transformed = preprocessor.fit_transform(X_train)\n    X_valid_transformed = preprocessor.transform(X_valid)\n    \n    # Model\n    model = AdaBoostRegressor(\n        base_estimator=DecisionTreeRegressor(max_depth=best_params['tree_depth'],\n                                             min_samples_split=best_params['min_samples_split']),\n        n_estimators=best_params['n_estimators'],\n        learning_rate=best_params['learning_rate'],\n        loss=best_params['loss'],\n        random_state=42\n    )\n    model.fit(X_train_transformed, y_train)\n    \n    # Predictions (log transformed)\n    log_oof_preds = model.predict(X_valid_transformed)\n    \n    # Revert log transformation\n    oof_predictions[valid_idx] = np.expm1(log_oof_preds)\n\n    fold_rmsle = np.sqrt(mean_squared_log_error(y.iloc[valid_idx], oof_predictions[valid_idx]))\n    print(f\"Fold {fold + 1} RMSLE: {fold_rmsle:.4f}\")\n    \n    # Store fold results\n    fold_result = pd.DataFrame({\n        'ID': X.index[valid_idx],\n        'Actual': y.iloc[valid_idx],\n        'OOF_Pred': oof_predictions[valid_idx],\n        'Fold': fold + 1\n    })\n    fold_oof_results.append(fold_result)\n    \n    # Test set predictions (transform test data)\n    X_test_transformed = preprocessor.transform(X_test)\n    log_test_preds = model.predict(X_test_transformed)\n    test_preds_per_fold[:, fold] = np.expm1(log_test_preds)\n\n# Combine fold results\noof_results_df = pd.concat(fold_oof_results, axis=0, ignore_index=True)\n\n# Average predictions on test data\nfinal_test_predictions = test_preds_per_fold.mean(axis=1)\n\n# Evaluate OOF predictions\noof_mse = mean_squared_error(y, oof_predictions)\noof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n\nprint(f\"OOF Mean Squared Error: {oof_mse:.4f}\")\nprint(f\"OOF Root Mean Squared Log Error: {oof_rmsle:.4f}\")\n\n# Output predictions\nprint(\"Final Test Predictions:\", final_test_predictions)\nprint(oof_results_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:38:12.249371Z","iopub.execute_input":"2024-12-30T06:38:12.249719Z","execution_failed":"2024-12-30T08:57:05.793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyper parameter code for Aada Boost\n\n# import pandas as pd\n# import numpy as np\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.impute import SimpleImputer\n# from sklearn.ensemble import AdaBoostRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_squared_error\n# import optuna\n\n# # Assuming train_df is already loaded\n# # Replace this with your actual data loading code\n# # train_df = pd.read_csv(\"your_data.csv\")\n\n# # Sample DataFrame (replace with actual dataset)\n# data = {\n#     \"feature1\": [1, 2, 3, np.nan, 5],\n#     \"feature2\": [\"A\", \"B\", \"A\", \"B\", np.nan],\n#     \"target\": [10, 15, 10, 25, 30]\n# }\n# train_df = pd.DataFrame(data)\n\n# # Define features\n# categorical_features = [col for col in train_df.columns if train_df[col].dtype == \"object\"]\n# numerical_features = [col for col in train_df.columns if train_df[col].dtype in [\"int64\", \"float64\"] and col != \"target\"]\n\n# # Preprocessor\n# numerical_transformer = Pipeline(steps=[\n#     (\"imputer\", SimpleImputer(strategy=\"mean\")),\n#     (\"scaler\", StandardScaler())\n# ])\n\n# categorical_transformer = Pipeline(steps=[\n#     (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n#     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n# ])\n\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         (\"num\", numerical_transformer, numerical_features),\n#         (\"cat\", categorical_transformer, categorical_features)\n#     ]\n# )\n\n# # Objective function\n# def objective(trial):\n#     X = train_df.drop(columns=[\"target\"])\n#     y = train_df[\"target\"]\n\n#     # Log-transform target to stabilize variance\n#     y_log = np.log1p(y)\n\n#     # Train-test split\n#     X_train, X_valid, y_train, y_valid = train_test_split(X, y_log, test_size=0.2, random_state=42)\n\n#     # Suggest hyperparameters\n#     base_estimator = DecisionTreeRegressor(\n#         max_depth=trial.suggest_int(\"tree_depth\", 1, 10),\n#         min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10)\n#     )\n\n#     params = {\n#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 1.0, log=True),\n#         \"loss\": trial.suggest_categorical(\"loss\", [\"linear\", \"square\", \"exponential\"]),\n#         \"base_estimator\": base_estimator\n#     }\n\n#     # Model pipeline\n#     model_pipeline = Pipeline(steps=[\n#         (\"preprocessor\", preprocessor),\n#         (\"regressor\", AdaBoostRegressor(**params))\n#     ])\n\n#     # Fit the model\n#     try:\n#         model_pipeline.fit(X_train, y_train)\n#         preds = model_pipeline.predict(X_valid)\n#         mse = mean_squared_error(y_valid, preds)\n#         return mse\n#     except Exception as e:\n#         print(f\"Error during model fitting: {e}\")\n#         raise\n\n# # Optimize\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=50)\n\n# print(\"Best parameters:\", study.best_params)\n# print(\"Best score:\", study.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:37:14.863088Z","iopub.status.idle":"2024-12-30T06:37:14.863571Z","shell.execute_reply":"2024-12-30T06:37:14.863355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_squared_log_error\n# from sklearn.ensemble import AdaBoostRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.impute import SimpleImputer\n# import optuna\n\n# # Log transform function\n# def log_transform(y):\n#     return np.log1p(y)  # log(1 + y) for zero-safe logarithm\n\n# # Inverse log transform function\n# def inverse_log_transform(y_log):\n#     return np.expm1(y_log)  # exp(y_log) - 1 for reverse transformation\n\n# # Check if 'Premium Amount' column is present in train_df\n# if 'Premium Amount' not in train_df.columns:\n#     raise KeyError(\"'Premium Amount' column is missing from the DataFrame\")\n\n# # Separate categorical and numeric columns\n# categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns\n# numeric_cols = train_df.select_dtypes(include=['number']).columns\n\n# # Impute missing values separately\n# num_imputer = SimpleImputer(strategy='mean')  # Imputer for numeric data\n# cat_imputer = SimpleImputer(strategy='most_frequent')  # Imputer for categorical data\n\n# # Handle features (X)\n# X = train_df.drop('Premium Amount', axis=1)\n# X_numeric = pd.DataFrame(num_imputer.fit_transform(X[numeric_cols]), columns=numeric_cols)\n# X_categorical = pd.DataFrame(cat_imputer.fit_transform(X[categorical_cols]), columns=categorical_cols)\n# X = pd.concat([X_numeric, X_categorical], axis=1)\n\n# # Handle target (y)\n# y = train_df['Premium Amount']\n# y = num_imputer.fit_transform(y.values.reshape(-1, 1)).ravel()  # Impute missing values in the target\n\n# # Log transform the target variable\n# y_log = log_transform(y)\n\n# def objective(trial):\n#     # Hyperparameter space to search\n#     params = {\n#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000, step=50),\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 1.0, log=True),\n#         \"base_estimator\": DecisionTreeRegressor(\n#             max_depth=trial.suggest_int(\"max_depth\", 1, 10),\n#             min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 20),\n#             min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 20),\n#         )\n#     }\n    \n#     # Split data into train and validation sets\n#     X_train, X_valid, y_train, y_valid = train_test_split(X, y_log, test_size=0.2, random_state=42)\n    \n#     # Initialize the model with the given parameters\n#     model = AdaBoostRegressor(**params)\n    \n#     # Fit the model on log-transformed target\n#     model.fit(X_train, y_train)\n    \n#     # Predict on the validation set and inverse transform\n#     y_pred_log = model.predict(X_valid)\n#     y_pred = inverse_log_transform(y_pred_log)\n#     y_valid_original = inverse_log_transform(y_valid)\n    \n#     # Calculate RMSLE on original scale\n#     score = np.sqrt(mean_squared_log_error(y_valid_original, y_pred))\n    \n#     return score\n\n# # Optuna Study for Hyperparameter Tuning\n# study = optuna.create_study(direction=\"minimize\")  # We want to minimize RMSLE\n# study.optimize(objective, n_trials=50)\n\n# # Print the best parameters and score\n# print(\"Best RMSLE:\", study.best_value)\n# print(\"Best Hyperparameters:\", study.best_params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:37:14.856394Z","iopub.status.idle":"2024-12-30T06:37:14.856846Z","shell.execute_reply":"2024-12-30T06:37:14.8567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(train_df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:37:14.858298Z","iopub.status.idle":"2024-12-30T06:37:14.858785Z","shell.execute_reply":"2024-12-30T06:37:14.858587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# model_params = {'n_estimators': 7000,\n#                 'learning_rate': 0.0037894924041441663,\n#                 'max_depth': 9,\n#                 'min_child_weight': 7,\n#                 'gamma': 0.003166869962093635,\n#                 'subsample': 0.8491559357878403,\n#                 'colsample_bytree': 0.9931791435553496,\n#                 'reg_alpha': 8.030670352805062e-08,\n#                 'reg_lambda': 0.23939002451629704,\n#                 \"random_state\": 42,\n#                 'eval_metric': 'rmsle',\n#                 \"objective\": \"reg:squarederror\",  # XGBoost objective for regression\n#                 \"tree_method\": \"hist\",  # Use GPU acceleration\n#                 \"device\": \"cuda\",\n#                 \"enable_categorical\": True  # Enable categorical handling if required\n#                }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:37:14.859787Z","iopub.status.idle":"2024-12-30T06:37:14.860278Z","shell.execute_reply":"2024-12-30T06:37:14.860081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_squared_log_error\n# from sklearn.ensemble import AdaBoostRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.impute import SimpleImputer\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# import optuna\n\n# # Log transform function\n# def log_transform(y):\n#     return np.log1p(y)  # log(1+y) to handle zero values\n\n# # Inverse log transform function\n# def inverse_log_transform(y_log):\n#     return np.expm1(y_log)  # exp(y) - 1 to revert log1p\n\n# # Prepare data\n# X = train_df.drop('Premium Amount', axis=1)\n# y = train_df['Premium Amount']\n\n# # Log transform the target variable\n# y_log = log_transform(y)\n\n# # Identify categorical and numerical features\n# categorical_features = [col for col in X.columns if X[col].dtype == 'object']\n# numerical_features = [col for col in X.columns if col not in categorical_features]\n\n# # Imputation transformers for numerical and categorical features\n# numerical_transformer = SimpleImputer(strategy='mean')  # Mean imputation for numerical features\n# categorical_transformer = SimpleImputer(strategy='most_frequent')  # Most frequent imputation for categorical features\n\n# # Create a column transformer to apply different imputation strategies\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_features),  # Apply mean imputation to numerical columns\n#         ('cat', categorical_transformer, categorical_features)  # Apply most frequent imputation to categorical columns\n#     ])\n\n# def objective(trial):\n#     # Hyperparameter space to search for AdaBoostRegressor\n#     params = {\n#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),  # Number of boosting iterations\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 1.0, log=True),  # Learning rate for boosting\n#         \"loss\": trial.suggest_categorical(\"loss\", [\"linear\", \"square\", \"exponential\"]),  # Loss function for AdaBoost\n#         \"base_estimator\": DecisionTreeRegressor(\n#             max_depth=trial.suggest_int(\"tree_depth\", 1, 10),  # Depth of the base decision tree estimator\n#             min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10)  # Minimum samples required to split a node\n#         ),\n#     }\n\n#     # Create a pipeline with preprocessing (imputation) and AdaBoostRegressor\n#     model_pipeline = Pipeline(steps=[\n#         ('preprocessor', preprocessor),  # Apply the preprocessor to impute missing values\n#         ('regressor', AdaBoostRegressor(**params))  # Apply the AdaBoost model\n#     ])\n    \n#     # Split data into train and validation sets\n#     X_train, X_valid, y_train, y_valid = train_test_split(X, y_log, test_size=0.2, random_state=42)\n\n#     # Fit the model on log-transformed target\n#     model_pipeline.fit(X_train, y_train)\n\n#     # Predict on the validation set and inverse transform\n#     y_pred_log = model_pipeline.predict(X_valid)\n#     y_pred = inverse_log_transform(y_pred_log)\n#     y_valid_original = inverse_log_transform(y_valid)\n\n#     # Calculate RMSLE on original scale\n#     score = np.sqrt(mean_squared_log_error(y_valid_original, y_pred))\n\n#     return score\n\n# # Optuna Study for Hyperparameter Tuning\n# study = optuna.create_study(direction=\"minimize\")  # We want to minimize RMSLE\n# study.optimize(objective, n_trials=50)\n\n# # Print the best parameters and score\n# print(\"Best RMSLE:\", study.best_value)\n# print(\"Best Hyperparameters:\", study.best_params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:37:14.861161Z","iopub.status.idle":"2024-12-30T06:37:14.86163Z","shell.execute_reply":"2024-12-30T06:37:14.861418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}